services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-continuuai}
      POSTGRES_USER: ${POSTGRES_USER:-continuuai}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-continuuai}
    ports:
      - "${POSTGRES_PORT:-5433}:5432"
    restart: ${RESTART_POLICY:-unless-stopped}
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U continuuai -d continuuai"]
      interval: 2s
      timeout: 2s
      retries: 30

  migrate:
    build:
      context: .
      dockerfile: services/migrate/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgres://continuuai:continuuai@postgres:5432/continuuai}
    restart: on-failure
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./migrations:/app/migrations:ro
    command: ["--migrations", "/app/migrations"]

  seed:
    build:
      context: .
      dockerfile: services/seed/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgres://continuuai:continuuai@postgres:5432/continuuai}
      ORG_ID: ${ORG_ID:-00000000-0000-0000-0000-000000000000}
      ORG_SLUG: ${ORG_SLUG:-example}
    restart: on-failure
    depends_on:
      migrate:
        condition: service_completed_successfully

  retrieval:
    build:
      context: .
      dockerfile: services/retrieval/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgres://continuuai:continuuai@postgres:5432/continuuai}
      ORG_ID: ${ORG_ID:-00000000-0000-0000-0000-000000000000}
      EMBEDDING_URL: ${EMBEDDING_URL:-http://embedding:8080}
      ADMIN_DEBUG_TOKEN: ${ADMIN_DEBUG_TOKEN:-debug_token}
      ENV: ${ENV:-production}
      # Retrieval knobs (use .env defaults if present)
      SEED_K: ${SEED_K:-40}
      HOP_DEPTH: ${HOP_DEPTH:-2}
      HOP_FANOUT: ${HOP_FANOUT:-80}
      FINAL_K: ${FINAL_K:-12}
      ALPHA_VEC: ${ALPHA_VEC:-0.55}
      BETA_BM25: ${BETA_BM25:-0.25}
      GAMMA_GRAPH: ${GAMMA_GRAPH:-0.15}
      DELTA_RECENCY: ${DELTA_RECENCY:-0.05}
      RECENCY_HALFLIFE_DAYS: ${RECENCY_HALFLIFE_DAYS:-45}
      USE_MMR: ${USE_MMR:-true}
      MMR_LAMBDA: ${MMR_LAMBDA:-0.7}
      MMR_POOL: ${MMR_POOL:-100}
      GRAPH_BONUS_MAP: ${GRAPH_BONUS_MAP:-}
    depends_on:
      postgres:
        condition: service_healthy
      seed:
        condition: service_completed_successfully
    ports:
      - "8081:8080"
    restart: ${RESTART_POLICY:-unless-stopped}

  inference:
    build:
      context: .
      dockerfile: services/inference/Dockerfile
    network_mode: host
    environment:
      RESPONSE_SCHEMA_PATH: /app/schemas/response-contract.v1.json
      LLAMA_SERVER_URL: ${LLAMA_SERVER_URL:-http://127.0.0.1:8084}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE:-0.3}
      LLM_MAX_TOKENS: ${LLM_MAX_TOKENS:-1024}
      USE_STUB: ${USE_STUB:-false}
    volumes:
      - ./schemas:/app/schemas:ro
    restart: ${RESTART_POLICY:-unless-stopped}

  api:
    build:
      context: .
      dockerfile: services/api-gateway/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgres://continuuai:continuuai@postgres:5432/continuuai}
      RETRIEVAL_URL: ${RETRIEVAL_SERVICE_URL:-http://retrieval:8080}
      INFERENCE_URL: ${INFERENCE_URL:-http://host.docker.internal:8082}
      RESPONSE_SCHEMA_PATH: /app/schemas/response-contract.v1.json
      ADMIN_TOKEN: ${ADMIN_TOKEN:-admin_secret}
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost:3000,http://localhost:3001}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./schemas:/app/schemas:ro
    depends_on:
      postgres:
        condition: service_healthy
      retrieval:
        condition: service_started
      inference:
        condition: service_started
    ports:
      - "${API_PORT:-8080}:8080"
    restart: ${RESTART_POLICY:-unless-stopped}

  graph-deriver:
    build:
      context: .
      dockerfile: services/graph-deriver/Dockerfile
    environment:
      DB_HOST: postgres
      DB_PORT: "5432"
      DB_NAME: ${POSTGRES_DB:-continuuai}
      DB_USER: ${POSTGRES_USER:-continuuai}
      DB_PASS: ${POSTGRES_PASSWORD:-continuuai}
      POLL_INTERVAL_SEC: ${POLL_INTERVAL_SEC:-10}
    restart: ${RESTART_POLICY:-unless-stopped}
    depends_on:
      postgres:
        condition: service_healthy
      migrate:
        condition: service_completed_successfully
      seed:
        condition: service_completed_successfully

  embedding:
    build:
      context: .
      dockerfile: services/embedding/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgres://continuuai:continuuai@postgres:5432/continuuai}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      EMBEDDING_VERSION: ${EMBEDDING_VERSION:-v1}
    depends_on:
      postgres:
        condition: service_healthy
      seed:
        condition: service_completed_successfully
    ports:
      - "8083:8080"
    restart: ${RESTART_POLICY:-unless-stopped}

  # ============================================================
  # Frontend Services (Phase 2-3)
  # ============================================================

  admin-dashboard:
    build:
      context: services/admin-dashboard
      dockerfile: Dockerfile
    environment:
      NEXT_PUBLIC_API_URL: http://localhost:${API_PORT:-8080}
      ADMIN_TOKEN: ${ADMIN_TOKEN:-admin_secret}
    ports:
      - "${ADMIN_DASHBOARD_PORT:-3001}:3000"
    depends_on:
      - api
    restart: ${RESTART_POLICY:-unless-stopped}

  user-app:
    build:
      context: services/user-app
      dockerfile: Dockerfile
    environment:
      NEXT_PUBLIC_API_URL: http://localhost:${API_PORT:-8080}
      SERVER_API_URL: http://api:8080
    ports:
      - "${USER_APP_PORT:-3000}:3000"
    depends_on:
      - api
    restart: ${RESTART_POLICY:-unless-stopped}

  # ============================================================
  # Ollama LLM Service (Phase 5 - requires GPU hardware)
  # Uncomment when GPU hardware arrives
  # ============================================================

  # ollama:
  #   image: ollama/ollama:latest
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_models:/root/.ollama
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   restart: ${RESTART_POLICY:-unless-stopped}

volumes:
  pgdata: {}
  # ollama_models: {}  # Uncomment with ollama service

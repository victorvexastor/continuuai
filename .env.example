# ContinuuAI Configuration
# Copy this file to .env and customize for your environment

# ========================================
# Core System
# ========================================
COMPOSE_PROJECT_NAME=continuuai
ENV=production

# ========================================
# Database
# ========================================
POSTGRES_DB=continuuai
POSTGRES_USER=continuuai
POSTGRES_PASSWORD=continuuai_secure_password_change_me
DATABASE_URL=postgres://continuuai:continuuai_secure_password_change_me@postgres:5432/continuuai

# External access (if needed)
POSTGRES_PORT=5433

# ========================================
# Organization Setup
# ========================================
ORG_ID=00000000-0000-0000-0000-000000000000
ORG_SLUG=example

# ========================================
# Embedding Service
# ========================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_VERSION=v1
EMBEDDING_URL=http://embedding:8080

# ========================================
# Retrieval Tuning Knobs
# ========================================
# Hybrid search weights (must sum to ~1.0)
ALPHA_VEC=0.55          # Vector similarity weight
BETA_BM25=0.25          # Lexical (BM25) weight
GAMMA_GRAPH=0.15        # Graph relationship weight
DELTA_RECENCY=0.05      # Recency decay weight

# Retrieval parameters
SEED_K=40               # Initial vector search results
HOP_DEPTH=2             # Graph traversal depth
HOP_FANOUT=80           # Max relationships per hop
FINAL_K=12              # Final results returned

# MMR (diversity)
USE_MMR=true
MMR_LAMBDA=0.7          # Balance relevance vs diversity (0=diverse, 1=relevant)
MMR_POOL=100            # Candidate pool size for MMR

# Recency decay
RECENCY_HALFLIFE_DAYS=45  # Days for relevance to decay by 50%

# Graph bonuses (optional, JSON map)
GRAPH_BONUS_MAP={}

# ========================================
# Inference Service (LLM)
# ========================================
INFERENCE_URL=http://inference:8080
RESPONSE_SCHEMA_PATH=/app/schemas/response-contract.v1.json

# LLM Configuration (used when real LLM is connected)
LLM_MODEL=mistral:7b    # Ollama model name
LLM_TEMPERATURE=0.3     # Lower = more deterministic
LLM_MAX_TOKENS=1024
LLM_TIMEOUT=30          # seconds

# Ollama service (when GPU hardware arrives)
OLLAMA_HOST=http://ollama:11434
OLLAMA_GPU_LAYERS=999   # Use all GPU layers

# ========================================
# API Gateway
# ========================================
API_PORT=8080
RETRIEVAL_SERVICE_URL=http://retrieval:8080

# ========================================
# Admin Dashboard
# ========================================
ADMIN_DASHBOARD_PORT=3001
ADMIN_TOKEN=admin_secret_token_change_me  # For /admin/* endpoints

# ========================================
# User-Facing App
# ========================================
USER_APP_PORT=3000
NEXT_PUBLIC_API_URL=http://localhost:8080

# ========================================
# Graph Deriver
# ========================================
POLL_INTERVAL_SEC=10    # How often to check for new events

# ========================================
# Observability
# ========================================
LOG_LEVEL=INFO          # DEBUG, INFO, WARNING, ERROR
ADMIN_DEBUG_TOKEN=debug_token_change_me  # For /debug/* endpoints

# ========================================
# Security
# ========================================
# Set to true to require authentication (future)
REQUIRE_AUTH=false
JWT_SECRET=your_jwt_secret_change_me

# CORS (for frontend development)
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ========================================
# Deployment
# ========================================
# Container restart policy
RESTART_POLICY=unless-stopped

# Resource limits (optional, uncomment to enable)
# CPU_LIMIT=2
# MEMORY_LIMIT=4g

# ========================================
# Development
# ========================================
# Set to true for hot reload, debug logging
DEV_MODE=false

# Test data
SEED_TEST_DATA=true
